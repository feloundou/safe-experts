Warning: Log dir /home/tyna/Documents/safe-experts/data/valor-anonymous-expert/valor-anonymous-expert_s0 already exists! Storing info there anyway.
here is the output file:  <_io.TextIOWrapper name='/home/tyna/Documents/safe-experts/data/valor-anonymous-expert/valor-anonymous-expert_s0/progress.txt' mode='w' encoding='UTF-8'>
[32;1mLogging data to /home/tyna/Documents/safe-experts/data/valor-anonymous-expert/valor-anonymous-expert_s0/progress.txt[0m
[36;1mSaving config:
[0m
{
    "ac_kwargs":	{
        "hidden_dims":	[
            64,
            64
        ]
    },
    "actor_critic":	"ActorCritic",
    "clip_ratio":	0.2,
    "composite_name":	"new_valor_penalized_standard",
    "con_dim":	10,
    "config_name":	"standard",
    "cost_lim":	25,
    "dc_kwargs":	{
        "hidden_dims":	64
    },
    "dc_lr":	0.0005,
    "disc":	"ValorDiscriminator",
    "env_fn":	"<function <lambda> at 0x7fd7d80903a0>",
    "episodes_per_epoch":	5,
    "epochs":	1000,
    "exp_name":	"valor-anonymous-expert",
    "gamma":	0.99,
    "k":	1,
    "lam":	0.97,
    "logger":	{
        "<utils.EpochLogger object at 0x7fd8595ad3a0>":	{
            "epoch_dict":	{},
            "exp_name":	"valor-anonymous-expert",
            "first_row":	true,
            "log_current_row":	{},
            "log_headers":	[],
            "output_dir":	"/home/tyna/Documents/safe-experts/data/valor-anonymous-expert/valor-anonymous-expert_s0",
            "output_file":	{
                "<_io.TextIOWrapper name='/home/tyna/Documents/safe-experts/data/valor-anonymous-expert/valor-anonymous-expert_s0/progress.txt' mode='w' encoding='UTF-8'>":	{
                    "mode":	"w"
                }
            }
        }
    },
    "logger_kwargs":	{
        "exp_name":	"valor-anonymous-expert",
        "output_dir":	"/home/tyna/Documents/safe-experts/data/valor-anonymous-expert/valor-anonymous-expert_s0"
    },
    "max_ep_len":	1000,
    "penalty_init":	1.0,
    "penalty_lr":	0.005,
    "pi_lr":	0.0003,
    "save_freq":	10,
    "seed":	0,
    "train_dc_interv":	10,
    "train_dc_iters":	10,
    "train_pi_iters":	1,
    "train_v_iters":	80,
    "vf_lr":	0.001
}
/home/tyna/.local/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[32;1m
Number of parameters: 	 pi: 8836, 	 v: 8769, 	 d: 4554
[0m
context distribution: Categorical(logits: torch.Size([10]))
context sample:  tensor(8)
WOW
tensor([[[-5.8663e-01, -5.0066e-02,  1.6093e-01,  5.4902e-01,  1.6503e-01,
          -4.8397e-01, -1.0299e-01,  3.9534e-02, -1.7700e-02, -5.3431e-02,
           1.0028e-01, -2.8233e-01, -2.0971e-02, -3.7303e-01,  6.2578e-02,
          -1.2853e-01,  1.7470e-01,  4.8732e-01, -3.1172e-01,  2.5634e-01,
          -2.9840e-02,  6.1582e-02, -6.0848e-01,  3.0799e-02, -3.0604e-01,
          -2.2922e-01,  7.4514e-03,  3.1968e-01, -3.9077e-02, -6.6558e-02,
           2.5217e-01,  1.3432e-01, -1.8394e-01,  1.8592e-01, -1.7909e-01,
          -1.6609e-01, -4.0169e-01, -3.6933e-01, -1.6482e-01,  2.6974e-01,
          -6.9006e-02,  2.8161e-01, -1.8815e-01, -1.0803e-01,  1.4522e-01,
           1.6175e-01,  2.7598e-01,  3.7878e-01, -3.1765e-01, -6.0348e-02,
          -4.6689e-01, -2.5445e-01, -4.0013e-01, -4.0797e-02, -2.8046e-01,
          -3.0366e-01,  3.2498e-01, -1.0036e-03, -1.2828e-01,  6.0136e-02,
           4.7242e-01, -2.4559e-01,  3.2599e-01,  1.9911e-02],
         [-7.4515e-02, -7.7979e-02, -1.8730e-01,  2.4538e-01, -1.1220e-01,
          -9.7191e-02,  1.7951e-01, -2.0172e-01,  1.6581e-01, -1.4781e-01,
           2.1596e-01,  5.8686e-02, -3.5844e-02,  8.3793e-02, -2.6896e-01,
           2.9829e-02,  1.0414e-01,  1.7318e-01,  1.7302e-01,  1.1117e-01,
           8.0354e-02,  4.9591e-01, -1.4905e-01, -1.1802e-01, -2.2369e-02,
           2.9508e-02,  3.6967e-01, -2.7851e-01, -1.5466e-01, -1.5508e-02,
           6.5329e-02, -8.3806e-02,  1.1494e-01,  2.1954e-01, -4.0309e-02,
          -1.5369e-02, -5.1375e-02, -5.9893e-02, -9.3366e-02,  2.6585e-01,
           2.4335e-01,  2.8862e-02,  8.1816e-02,  2.4257e-01, -2.8886e-01,
          -9.1472e-02,  7.9627e-02,  3.9040e-01, -1.9986e-01, -1.8443e-02,
           6.9526e-02, -9.3954e-02, -1.7247e-01, -1.6311e-01,  1.5884e-01,
          -1.2489e-01,  7.0704e-03,  1.4259e-01,  1.7652e-02, -3.1975e-01,
          -1.8709e-01, -3.1929e-02,  6.3153e-02,  1.5557e-02],
         [ 6.5679e-01, -4.7439e-01, -5.2241e-01, -4.9286e-01, -1.4747e-01,
           3.8668e-01,  2.9638e-01, -3.4992e-01, -8.0824e-01,  2.3480e-01,
          -4.5810e-01, -1.2269e-01, -8.1453e-01,  6.9215e-01, -1.3783e-01,
           3.4794e-01,  3.5277e-02, -5.0093e-01,  1.5473e-01,  1.3955e-01,
           1.1766e-01, -2.8492e-01,  8.8467e-01,  2.9914e-01,  1.1212e-01,
          -7.1624e-02, -4.0003e-01,  4.4119e-02,  3.3677e-01,  4.8563e-01,
          -6.8370e-01, -7.3959e-01, -6.4536e-02, -4.3885e-02,  2.4002e-01,
          -2.6384e-01,  3.2752e-02,  5.9513e-01,  6.0660e-01,  2.8908e-01,
           2.7224e-01, -3.3330e-02, -4.6989e-02, -4.4581e-02, -4.4029e-01,
           1.2246e-01, -5.0027e-01, -1.6663e-01,  5.5397e-01,  1.0030e-01,
           7.2894e-01,  3.7825e-01,  7.3696e-01,  3.4664e-01, -1.6559e-01,
           4.4641e-01,  2.7772e-01, -3.1534e-01,  2.4732e-01,  6.2425e-02,
          -6.4828e-01, -5.2492e-01, -3.2784e-01, -9.1942e-02],
         [ 3.7709e-01, -1.7687e-02, -4.7589e-01, -6.2143e-02,  5.4679e-01,
          -5.4768e-02, -2.5694e-01,  4.5885e-01,  3.7434e-01, -3.2425e-01,
          -2.2072e-01,  1.3880e-01,  3.5511e-01,  2.0354e-01, -6.5141e-01,
          -7.6728e-02, -3.4673e-01,  3.1986e-01, -6.3317e-01, -1.2263e-01,
          -4.9100e-02,  5.3718e-01, -2.3706e-01,  3.5605e-01, -5.3229e-01,
          -1.5182e-01,  2.5661e-01, -3.8540e-01, -4.9644e-01, -1.2213e-01,
           4.5670e-01,  2.3567e-01, -2.6610e-01, -3.4910e-01,  1.2018e-01,
          -2.8445e-01,  4.9716e-01,  9.1305e-03,  2.5303e-01, -3.4518e-01,
          -3.5469e-01, -5.1210e-01,  3.4665e-01,  3.2082e-01, -3.7706e-02,
          -3.5332e-01,  2.0143e-01,  3.9856e-01,  2.2665e-01,  1.0880e-02,
          -1.6337e-01,  2.4129e-01, -4.9604e-01, -3.1456e-01,  3.1043e-01,
          -2.9788e-01, -5.9511e-02,  5.6905e-01, -3.5689e-02, -4.2908e-01,
          -5.3188e-01,  4.1029e-01, -3.5204e-01, -4.9565e-01],
         [-3.5885e-01,  6.3076e-01,  2.0574e-01,  1.3212e-01, -4.0695e-01,
           8.8030e-02,  1.2540e-01, -8.2301e-01, -5.2214e-01, -1.2937e-02,
           4.5322e-01, -3.6681e-01, -5.4437e-01, -1.9595e-01,  4.2837e-01,
           2.9169e-01,  4.0966e-01, -8.1109e-02,  4.6207e-01,  1.4521e-01,
          -1.5447e-01, -3.8851e-01, -8.6928e-02, -5.1281e-01,  3.0710e-01,
           3.4236e-01, -9.2377e-02,  1.4614e-01,  4.0834e-01,  3.6268e-01,
          -3.8733e-01,  7.4701e-02,  4.8794e-02,  5.6108e-01, -1.7997e-01,
           2.8972e-01, -7.5282e-01,  1.4117e-01, -2.4171e-01,  7.2740e-01,
           2.8334e-01,  8.1407e-01, -4.2871e-01, -4.0933e-01,  1.4591e-01,
           8.0296e-01, -2.9181e-01, -1.4379e-01, -2.6433e-01, -6.5882e-02,
           1.3499e-01, -2.4088e-01,  2.4024e-01,  4.3993e-01,  1.0580e-01,
           2.2925e-01,  6.4473e-01, -1.5735e-01, -4.7324e-01,  2.1989e-01,
           4.3124e-01, -5.1349e-01,  7.5705e-01,  5.1017e-01],
         [-8.3162e-01,  1.2360e+00,  1.2494e+00,  1.0641e+00, -1.1949e+00,
           3.7933e-02,  4.1958e-01, -1.2981e+00, -2.5092e-01, -2.0446e-01,
           5.8010e-01, -1.1029e+00, -1.0386e+00, -9.2834e-01,  1.1459e+00,
          -1.1097e-01,  8.4279e-01,  4.5003e-01,  1.5236e+00,  3.5805e-01,
          -7.0286e-01, -1.1252e+00,  3.3436e-01, -1.1054e+00,  4.3359e-01,
           5.6399e-01,  2.7711e-01,  3.8255e-01,  4.8747e-01,  3.2081e-01,
          -8.2339e-01,  8.1444e-01,  6.4724e-01,  1.2624e+00, -7.1624e-01,
           4.2056e-02, -1.4621e+00,  5.6222e-04, -7.9923e-01,  1.2363e+00,
           3.0964e-01,  1.2505e+00, -9.4476e-01, -1.4426e+00,  7.2699e-01,
           9.6189e-01, -5.8469e-01, -4.1403e-01, -6.6499e-01, -2.2666e-01,
           5.1978e-01, -8.1383e-01,  1.7976e-01,  9.7076e-01, -2.2301e-01,
           7.3550e-01,  9.8514e-01, -4.7429e-01, -4.8641e-01,  9.8108e-01,
           1.7374e+00, -5.9619e-01,  1.3549e+00,  7.9077e-01],
         [ 5.5200e-01,  7.4899e-01,  6.3224e-02, -2.7847e-01, -4.4598e-01,
           5.0643e-02, -4.1422e-01,  4.7923e-01,  6.2404e-01, -6.6536e-02,
          -1.0511e-01, -2.2571e-01,  4.9630e-01,  2.9000e-01, -2.0537e-03,
           3.0765e-03, -4.3462e-01,  1.0997e-01,  7.0004e-01, -3.0830e-01,
           3.0607e-01,  3.9551e-02,  5.6084e-01,  7.0659e-02, -3.0755e-01,
           7.0159e-01,  6.6899e-01, -7.3338e-01, -7.8915e-01, -1.4978e-02,
          -6.3828e-02,  6.1804e-01,  4.3372e-01, -3.5344e-01,  3.2878e-02,
           6.5024e-01,  4.5775e-01,  6.9840e-01,  7.9783e-02, -6.3812e-01,
          -7.1260e-02, -2.9812e-01,  8.2807e-02, -5.7014e-01,  2.2363e-01,
           2.5908e-01, -5.8611e-01, -4.1254e-01, -1.6264e-01, -1.2803e-01,
          -1.6770e-02,  1.0868e-01, -2.0706e-01, -4.2037e-01,  4.0592e-01,
           4.0751e-01, -4.8568e-01,  6.0410e-02, -3.4015e-01,  1.1174e-01,
          -5.1855e-01,  3.0254e-01, -4.5776e-01,  8.4044e-02],
         [ 3.7118e-01, -2.0718e+00, -1.7309e+00, -8.1596e-01,  1.4272e+00,
           8.3292e-02,  1.4607e-01,  5.1622e-01, -4.5452e-02,  5.5148e-02,
          -5.7020e-01,  1.0165e+00,  1.3043e-01,  8.6132e-01, -1.3979e+00,
           4.0617e-01, -5.3958e-01, -1.6880e-01, -2.1064e+00, -4.1754e-03,
           5.1305e-01,  1.4816e+00, -1.0907e+00,  1.1031e+00, -5.4705e-01,
          -8.8021e-01, -6.9887e-01,  1.9217e-02, -1.4626e-02, -3.7983e-01,
           9.6543e-01, -1.4018e+00, -9.3855e-01, -6.7329e-01,  5.7780e-01,
          -5.6945e-01,  9.8658e-01, -6.3951e-01,  8.9316e-01, -5.2663e-01,
           1.1100e-01, -1.0976e+00,  8.8916e-01,  1.9197e+00, -1.2424e+00,
          -8.4568e-01,  7.2051e-01,  1.0709e+00,  7.0884e-01,  5.2716e-02,
          -6.3279e-01,  5.3304e-01, -1.9058e-01, -7.9888e-01, -4.6139e-02,
          -1.1655e+00, -2.8584e-01,  5.7964e-01,  7.3355e-01, -1.2794e+00,
          -1.4316e+00, -3.7762e-02, -9.7107e-01, -1.1461e+00],
         [-8.7993e-01,  1.3426e+00,  1.2399e+00,  1.1259e+00, -1.1396e+00,
          -2.7709e-02,  4.1456e-01, -1.4041e+00, -5.0364e-01, -3.7933e-01,
           8.1360e-01, -1.0350e+00, -1.1746e+00, -9.2594e-01,  1.0297e+00,
          -1.6288e-01,  9.6680e-01,  3.7783e-01,  1.3107e+00,  3.5887e-01,
          -7.9842e-01, -1.1202e+00,  3.9717e-01, -1.2620e+00,  6.0939e-01,
           3.9822e-01,  3.5138e-01,  5.3752e-01,  4.3664e-01,  5.1601e-01,
          -8.0092e-01,  8.8160e-01,  5.3053e-01,  1.3361e+00, -7.4905e-01,
          -1.9869e-01, -1.5211e+00,  6.3267e-02, -8.5773e-01,  1.4280e+00,
           2.5237e-01,  1.3853e+00, -9.3360e-01, -1.3496e+00,  6.7334e-01,
           8.5644e-01, -4.7915e-01, -2.9247e-01, -7.4858e-01, -1.4028e-01,
           5.4506e-01, -8.8100e-01,  2.7038e-01,  9.7848e-01, -3.8213e-01,
           8.1364e-01,  1.1277e+00, -5.2540e-01, -4.1306e-01,  1.0241e+00,
           1.9690e+00, -6.3483e-01,  1.4523e+00,  8.0906e-01],
         [ 1.0189e+00, -1.1794e-01, -1.1017e+00, -7.6682e-01,  7.0624e-01,
          -4.0335e-01, -1.1007e+00,  1.9577e+00,  1.8445e+00, -2.9854e-01,
          -3.7711e-01,  7.1603e-01,  2.2518e+00,  7.5553e-01, -1.2172e+00,
          -8.4010e-02, -1.3205e+00,  3.3374e-01, -1.0207e+00, -7.5071e-01,
           1.1475e+00,  1.7174e+00, -6.7829e-01,  9.9527e-01, -9.0714e-01,
           3.1850e-01,  9.8713e-01, -1.2251e+00, -1.9777e+00, -7.8011e-01,
           1.4272e+00,  3.1935e-01, -2.1260e-01, -1.6887e+00,  4.8031e-01,
           9.0673e-01,  1.8814e+00,  2.2873e-01,  6.8943e-01, -2.2367e+00,
          -3.3973e-01, -1.5160e+00,  1.1356e+00,  1.1287e+00, -6.7630e-02,
          -7.8090e-01,  2.5772e-01,  2.1822e-01, -6.0900e-02, -2.0347e-01,
          -1.1430e+00,  5.5588e-01, -1.2402e+00, -1.8264e+00,  9.4981e-01,
          -7.9382e-01, -1.6712e+00,  7.0696e-01, -5.2044e-01, -1.1976e+00,
          -1.9805e+00,  1.4002e+00, -1.4518e+00, -6.2674e-01]]],
       grad_fn=<AddBackward0>)
INTER STATES
Traceback (most recent call last):
  File "/home/tyna/Documents/safe-experts/algos/train_expert_valor_penalized.py", line 281, in <module>
    valor_penalized(lambda: gym.make(args.env), actor_critic=ActorCritic, ac_kwargs=dict(hidden_dims=[args.hid] * args.l),
  File "/home/tyna/Documents/safe-experts/algos/train_expert_valor_penalized.py", line 221, in valor_penalized
    _, _, log_p = discrim(dc_diff, con)
  File "/home/tyna/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/tyna/Documents/safe-experts/algos/neural_nets.py", line 404, in forward
    pred, loggt, logp = self.pi(seq, gt)
  File "/home/tyna/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/tyna/Documents/safe-experts/algos/neural_nets.py", line 378, in forward
    inter_states = self.activation(self.fc1(seq))
TypeError: softmax() received an invalid combination of arguments - got (Tensor), but expected one of:
 * (Tensor input, name dim, *, torch.dtype dtype)
 * (Tensor input, int dim, torch.dtype dtype)

